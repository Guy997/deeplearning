{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Enter State Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:14.872027Z",
     "start_time": "2018-01-10T13:12:11.142437Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX TITAN X (0000:04:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "%matplotlib inline\n",
    "#path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "from importlib import reload  # Python 3\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:14.875442Z",
     "start_time": "2018-01-10T13:12:14.873443Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:15.122113Z",
     "start_time": "2018-01-10T13:12:14.876703Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "steps_per_epoch = int(np.ceil(batches.samples/batch_size))\n",
    "validation_steps = int(np.ceil(val_batches.samples/(batch_size*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:15.436217Z",
     "start_time": "2018-01-10T13:12:15.124480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using batches, we could just import all the data into an array to save some processing time. (In most examples I'm using the batches, however - just because that's how I happened to start out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:25.547744Z",
     "start_time": "2018-01-10T13:12:15.437637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:26.323309Z",
     "start_time": "2018-01-10T13:12:25.549422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/val.dat', val)\n",
    "save_array(path+'results/trn.dat', trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:26.853677Z",
     "start_time": "2018-01-10T13:12:26.324454Z"
    }
   },
   "outputs": [],
   "source": [
    "val = load_array(path+'results/val.dat')\n",
    "trn = load_array(path+'results/trn.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run sample experiments on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should find that everything that worked on the sample (see statefarm-sample.ipynb), works on the full dataset too. Only better! Because now we have more data. So let's see how they go - the models in this section are exact copies of the sample notebook models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:12:26.867239Z",
     "start_time": "2018-01-10T13:12:26.854928Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, steps_per_epoch, epochs=2, validation_data=val_batches, \n",
    "                     validation_steps=validation_steps)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, steps_per_epoch, epochs=4, validation_data=val_batches, \n",
    "                     validation_steps=validation_steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:13:45.348694Z",
     "start_time": "2018-01-10T13:12:26.868298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/24 [==============================] - 10s 433ms/step - loss: 1.5693 - acc: 0.5293 - val_loss: 2.2747 - val_acc: 0.2550\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 9s 367ms/step - loss: 0.3224 - acc: 0.9451 - val_loss: 1.8041 - val_acc: 0.2860\n",
      "Epoch 1/4\n",
      "24/24 [==============================] - 10s 413ms/step - loss: 0.1051 - acc: 0.9901 - val_loss: 1.9799 - val_acc: 0.3120\n",
      "Epoch 2/4\n",
      "24/24 [==============================] - 9s 367ms/step - loss: 0.0404 - acc: 0.9987 - val_loss: 2.1482 - val_acc: 0.3250\n",
      "Epoch 3/4\n",
      "24/24 [==============================] - 9s 367ms/step - loss: 0.0224 - acc: 0.9993 - val_loss: 2.2305 - val_acc: 0.3580\n",
      "Epoch 4/4\n",
      "24/24 [==============================] - 9s 369ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 2.2415 - val_acc: 0.3740\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, with no regularization or augmentation we're getting some reasonable results from our simple convolutional model. So with augmentation, we hopefully will see some very good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:13:45.462521Z",
     "start_time": "2018-01-10T13:13:45.351274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:15:31.014511Z",
     "start_time": "2018-01-10T13:13:45.464453Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/24 [==============================] - 19s 779ms/step - loss: 2.5529 - acc: 0.2210 - val_loss: 2.2162 - val_acc: 0.2770\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 15s 606ms/step - loss: 1.8697 - acc: 0.3692 - val_loss: 2.0072 - val_acc: 0.3230\n",
      "Epoch 1/4\n",
      "24/24 [==============================] - 19s 779ms/step - loss: 1.5607 - acc: 0.4863 - val_loss: 1.9515 - val_acc: 0.2840\n",
      "Epoch 2/4\n",
      "24/24 [==============================] - 15s 608ms/step - loss: 1.4616 - acc: 0.5177 - val_loss: 1.9731 - val_acc: 0.2970\n",
      "Epoch 3/4\n",
      "24/24 [==============================] - 15s 612ms/step - loss: 1.3418 - acc: 0.5453 - val_loss: 1.9866 - val_acc: 0.2150\n",
      "Epoch 4/4\n",
      "24/24 [==============================] - 15s 611ms/step - loss: 1.2361 - acc: 0.6133 - val_loss: 2.0294 - val_acc: 0.2290\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:19:17.660886Z",
     "start_time": "2018-01-10T13:15:31.017266Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "24/24 [==============================] - 19s 789ms/step - loss: 1.1746 - acc: 0.6082 - val_loss: 1.9254 - val_acc: 0.2940\n",
      "Epoch 2/15\n",
      "24/24 [==============================] - 14s 603ms/step - loss: 1.1016 - acc: 0.6433 - val_loss: 1.8997 - val_acc: 0.3220\n",
      "Epoch 3/15\n",
      "24/24 [==============================] - 14s 601ms/step - loss: 1.0579 - acc: 0.6516 - val_loss: 1.8398 - val_acc: 0.3060\n",
      "Epoch 4/15\n",
      "24/24 [==============================] - 15s 605ms/step - loss: 0.9796 - acc: 0.6855 - val_loss: 1.6889 - val_acc: 0.3800\n",
      "Epoch 5/15\n",
      "24/24 [==============================] - 15s 613ms/step - loss: 0.8753 - acc: 0.7147 - val_loss: 1.6203 - val_acc: 0.4120\n",
      "Epoch 6/15\n",
      "24/24 [==============================] - 15s 605ms/step - loss: 0.8660 - acc: 0.7215 - val_loss: 1.5431 - val_acc: 0.4470\n",
      "Epoch 7/15\n",
      "24/24 [==============================] - 15s 612ms/step - loss: 0.8319 - acc: 0.7426 - val_loss: 1.3342 - val_acc: 0.4820\n",
      "Epoch 8/15\n",
      "24/24 [==============================] - 15s 629ms/step - loss: 0.7438 - acc: 0.7661 - val_loss: 1.2683 - val_acc: 0.5270\n",
      "Epoch 9/15\n",
      "24/24 [==============================] - 15s 623ms/step - loss: 0.7320 - acc: 0.7742 - val_loss: 1.2446 - val_acc: 0.5110\n",
      "Epoch 10/15\n",
      "24/24 [==============================] - 15s 609ms/step - loss: 0.7029 - acc: 0.7821 - val_loss: 1.0673 - val_acc: 0.5990\n",
      "Epoch 11/15\n",
      "24/24 [==============================] - 15s 612ms/step - loss: 0.6689 - acc: 0.8022 - val_loss: 0.8389 - val_acc: 0.7270\n",
      "Epoch 12/15\n",
      "24/24 [==============================] - 15s 607ms/step - loss: 0.6544 - acc: 0.8124 - val_loss: 0.7727 - val_acc: 0.7460\n",
      "Epoch 13/15\n",
      "24/24 [==============================] - 14s 604ms/step - loss: 0.6200 - acc: 0.8234 - val_loss: 0.8189 - val_acc: 0.7270\n",
      "Epoch 14/15\n",
      "24/24 [==============================] - 15s 619ms/step - loss: 0.5998 - acc: 0.8015 - val_loss: 0.5809 - val_acc: 0.8090\n",
      "Epoch 15/15\n",
      "24/24 [==============================] - 15s 616ms/step - loss: 0.5655 - acc: 0.8213 - val_loss: 0.5515 - val_acc: 0.8300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c5003b240>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, steps_per_epoch, epochs=15, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm shocked by *how* good these results are! We're regularly seeing 75-80% accuracy on the validation set, which puts us into the top third or better of the competition. With such a simple model and no dropout or semi-supervised learning, this really speaks to the power of this approach to data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four conv/pooling pairs + dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the results are still very unstable - the validation accuracy jumps from epoch to epoch. Perhaps a deeper model with some dropout would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:19:17.771634Z",
     "start_time": "2018-01-10T13:19:17.663199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:19:18.881613Z",
     "start_time": "2018-01-10T13:19:17.773251Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Conv2D(32,(3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64,(3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(128,(3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:19:18.893285Z",
     "start_time": "2018-01-10T13:19:18.883024Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:20:00.290263Z",
     "start_time": "2018-01-10T13:19:18.894549Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/24 [==============================] - 19s 782ms/step - loss: 3.5253 - acc: 0.1230 - val_loss: 2.2149 - val_acc: 0.2100\n",
      "Epoch 2/2\n",
      "24/24 [==============================] - 15s 616ms/step - loss: 3.1404 - acc: 0.1552 - val_loss: 2.2839 - val_acc: 0.2300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c7c2195f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch, epochs=2, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:20:00.295518Z",
     "start_time": "2018-01-10T13:20:00.292568Z"
    }
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:22:35.525850Z",
     "start_time": "2018-01-10T13:20:00.297629Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 19s 783ms/step - loss: 2.9247 - acc: 0.1787 - val_loss: 2.3075 - val_acc: 0.1550\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 19s 774ms/step - loss: 2.8073 - acc: 0.2099 - val_loss: 2.4846 - val_acc: 0.1530\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 12s 516ms/step - loss: 2.6097 - acc: 0.2414 - val_loss: 2.7364 - val_acc: 0.1500\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 15s 611ms/step - loss: 2.5821 - acc: 0.2543 - val_loss: 2.9833 - val_acc: 0.1280\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 15s 611ms/step - loss: 2.4576 - acc: 0.2951 - val_loss: 3.0379 - val_acc: 0.1200\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 15s 620ms/step - loss: 2.4471 - acc: 0.2884 - val_loss: 3.1420 - val_acc: 0.1150\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 15s 610ms/step - loss: 2.2162 - acc: 0.3383 - val_loss: 3.2096 - val_acc: 0.1180\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 15s 608ms/step - loss: 2.2098 - acc: 0.3363 - val_loss: 3.2340 - val_acc: 0.1370\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 15s 609ms/step - loss: 2.1079 - acc: 0.3555 - val_loss: 2.8120 - val_acc: 0.1690\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 15s 615ms/step - loss: 2.0698 - acc: 0.3661 - val_loss: 2.5486 - val_acc: 0.1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c530d7da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch, epochs=10, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:22:35.531446Z",
     "start_time": "2018-01-10T13:22:35.528355Z"
    }
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:25:16.359642Z",
     "start_time": "2018-01-10T13:22:35.533716Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 23s 950ms/step - loss: 1.9946 - acc: 0.3802 - val_loss: 2.2454 - val_acc: 0.2220\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 16s 677ms/step - loss: 1.9861 - acc: 0.3889 - val_loss: 1.9335 - val_acc: 0.2960\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 12s 507ms/step - loss: 1.8180 - acc: 0.4163 - val_loss: 1.7115 - val_acc: 0.3890\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 19s 776ms/step - loss: 1.8090 - acc: 0.4290 - val_loss: 1.5234 - val_acc: 0.4540\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 12s 495ms/step - loss: 1.7559 - acc: 0.4508 - val_loss: 1.4253 - val_acc: 0.4990\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 15s 628ms/step - loss: 1.7149 - acc: 0.4569 - val_loss: 1.2912 - val_acc: 0.5500\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 15s 609ms/step - loss: 1.6628 - acc: 0.4809 - val_loss: 1.1859 - val_acc: 0.6020\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 19s 779ms/step - loss: 1.6352 - acc: 0.4798 - val_loss: 1.0579 - val_acc: 0.6440\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 16s 672ms/step - loss: 1.6067 - acc: 0.5000 - val_loss: 0.9203 - val_acc: 0.6820\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 12s 507ms/step - loss: 1.5851 - acc: 0.4990 - val_loss: 0.8714 - val_acc: 0.7030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c530e6208>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, steps_per_epoch, epochs=10, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking quite a bit better - the accuracy is similar, but the stability is higher. There's still some way to go however..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagenet conv features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have so little data, and it is similar to imagenet images (full color photos), using pre-trained VGG weights is likely to be helpful - in fact it seems likely that we won't need to fine-tune the convolutional layer weights much, if at all. So we can pre-compute the output of the last convolutional layer, as we did in lesson 3 when we experimented with dropout. (However this means that we can't use full data augmentation, since we can't pre-compute something that changes every image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:25:18.795742Z",
     "start_time": "2018-01-10T13:25:16.362065Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:25:18.900700Z",
     "start_time": "2018-01-10T13:25:18.797313Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:25:19.212850Z",
     "start_time": "2018-01-10T13:25:18.902076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:25:19.322618Z",
     "start_time": "2018-01-10T13:25:19.215442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(path+'test', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:05.602388Z",
     "start_time": "2018-01-10T13:25:19.325044Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict_generator(batches, int(np.ceil(batches.samples/batch_size)))\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, int(np.ceil(val_batches.samples/(batch_size*2))))\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, int(np.ceil(test_batches.samples/(batch_size*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:06.273014Z",
     "start_time": "2018-01-10T13:26:05.604703Z"
    }
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:06.603399Z",
     "start_time": "2018-01-10T13:26:06.274219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512, 14, 14)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm dense layers on pretrained conv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've pre-computed the output of the last convolutional layer, we need to create a network that takes that as input, and predicts our 10 classes. Let's try using a simplified version of VGG's dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:06.608776Z",
     "start_time": "2018-01-10T13:26:06.604516Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:06.642997Z",
     "start_time": "2018-01-10T13:26:06.609895Z"
    }
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:07.146356Z",
     "start_time": "2018-01-10T13:26:06.644265Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:10.504803Z",
     "start_time": "2018-01-10T13:26:07.147614Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "1500/1500 [==============================] - 0s 211us/step - loss: 4.8353 - acc: 0.1073 - val_loss: 5.7340 - val_acc: 0.1140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b85cf8748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:10.508097Z",
     "start_time": "2018-01-10T13:26:10.506178Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:11.173910Z",
     "start_time": "2018-01-10T13:26:10.509317Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 0s 214us/step - loss: 4.0202 - acc: 0.1100 - val_loss: 3.5989 - val_acc: 0.0920\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 0s 209us/step - loss: 3.7136 - acc: 0.1373 - val_loss: 3.1136 - val_acc: 0.0730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b802b7128>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:11.212850Z",
     "start_time": "2018-01-10T13:26:11.175156Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try pre-computing 5 epochs worth of augmented data, so we can experiment with combining dropout and augmentation on the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-computed data augmentation + dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our usual data augmentation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:26:11.326672Z",
     "start_time": "2018-01-10T13:26:11.215657Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use those to create a dataset of convolutional features 5x bigger than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:14.617986Z",
     "start_time": "2018-01-10T13:26:11.329038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = conv_model.predict_generator(da_batches,  5*int(np.ceil((da_batches.samples)/(batch_size))), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:15.776067Z",
     "start_time": "2018-01-10T13:27:14.620189Z"
    }
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/da_conv_feat2.dat', da_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:16.589179Z",
     "start_time": "2018-01-10T13:27:15.777242Z"
    }
   },
   "outputs": [],
   "source": [
    "da_conv_feat = load_array(path+'results/da_conv_feat2.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the real training data as well in its non-augmented form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:17.325606Z",
     "start_time": "2018-01-10T13:27:16.590416Z"
    }
   },
   "outputs": [],
   "source": [
    "da_conv_feat = np.concatenate([da_conv_feat, conv_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now got a dataset 6x bigger than before, we'll need to copy our labels 6 times too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:17.329124Z",
     "start_time": "2018-01-10T13:27:17.327029Z"
    }
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on some experiments the previous model works well, with bigger dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:17.357181Z",
     "start_time": "2018-01-10T13:27:17.330272Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:17.398597Z",
     "start_time": "2018-01-10T13:27:17.358381Z"
    }
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:18.021213Z",
     "start_time": "2018-01-10T13:27:17.399749Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_da_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model as usual, with pre-computed augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:22.818593Z",
     "start_time": "2018-01-10T13:27:18.022723Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 1s 157us/step - loss: 4.1739 - acc: 0.1190 - val_loss: 1.7323 - val_acc: 0.4320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2b2bc978>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:22.821933Z",
     "start_time": "2018-01-10T13:27:22.820099Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:28.197156Z",
     "start_time": "2018-01-10T13:27:22.823165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "9000/9000 [==============================] - 1s 149us/step - loss: 3.0138 - acc: 0.1702 - val_loss: 1.5579 - val_acc: 0.6080\n",
      "Epoch 2/4\n",
      "9000/9000 [==============================] - 1s 148us/step - loss: 2.4110 - acc: 0.2206 - val_loss: 1.4365 - val_acc: 0.7230\n",
      "Epoch 3/4\n",
      "9000/9000 [==============================] - 1s 148us/step - loss: 2.0996 - acc: 0.2818 - val_loss: 1.3154 - val_acc: 0.7560\n",
      "Epoch 4/4\n",
      "9000/9000 [==============================] - 1s 148us/step - loss: 1.9277 - acc: 0.3307 - val_loss: 1.1721 - val_acc: 0.7630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c7c1a09b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, epochs=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:28.200358Z",
     "start_time": "2018-01-10T13:27:28.198507Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:33.610561Z",
     "start_time": "2018-01-10T13:27:28.201556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "9000/9000 [==============================] - 1s 149us/step - loss: 1.8020 - acc: 0.3896 - val_loss: 1.0272 - val_acc: 0.8130\n",
      "Epoch 2/4\n",
      "9000/9000 [==============================] - 1s 148us/step - loss: 1.7003 - acc: 0.4277 - val_loss: 0.9038 - val_acc: 0.8470\n",
      "Epoch 3/4\n",
      "9000/9000 [==============================] - 1s 148us/step - loss: 1.6278 - acc: 0.4706 - val_loss: 0.7905 - val_acc: 0.8730\n",
      "Epoch 4/4\n",
      "9000/9000 [==============================] - 1s 152us/step - loss: 1.5601 - acc: 0.4964 - val_loss: 0.7329 - val_acc: 0.8860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c7c1a0ac8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, epochs=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good - let's save those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:33.656561Z",
     "start_time": "2018-01-10T13:27:33.611917Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try using a combination of [pseudo labeling](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf) and [knowledge distillation](https://arxiv.org/abs/1503.02531) to allow us to use unlabeled data (i.e. do semi-supervised learning). For our initial experiment we'll use the validation set as the unlabeled data, so that we can see that it is working without using the test set. At a later date we'll try using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we simply calculate the predictions of our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:34.396822Z",
     "start_time": "2018-01-10T13:27:33.659496Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...concatenate them with our training labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:34.400572Z",
     "start_time": "2018-01-10T13:27:34.398283Z"
    }
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([da_trn_labels, val_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:35.269941Z",
     "start_time": "2018-01-10T13:27:34.401934Z"
    }
   },
   "outputs": [],
   "source": [
    "comb_feat = np.concatenate([da_conv_feat, conv_val_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and fine-tune our model using that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:35.286979Z",
     "start_time": "2018-01-10T13:27:35.271408Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:36.834110Z",
     "start_time": "2018-01-10T13:27:35.288379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 1.5338 - acc: 0.5402 - val_loss: 0.6742 - val_acc: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2b11cf98>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:42.835682Z",
     "start_time": "2018-01-10T13:27:36.835381Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 1.5131 - acc: 0.5492 - val_loss: 0.6520 - val_acc: 0.9070\n",
      "Epoch 2/4\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 1.4655 - acc: 0.5743 - val_loss: 0.6067 - val_acc: 0.9110\n",
      "Epoch 3/4\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 1.4394 - acc: 0.5821 - val_loss: 0.5799 - val_acc: 0.9040\n",
      "Epoch 4/4\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 1.4063 - acc: 0.6026 - val_loss: 0.5382 - val_acc: 0.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b802b74a8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, epochs=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:42.840104Z",
     "start_time": "2018-01-10T13:27:42.837587Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:48.804987Z",
     "start_time": "2018-01-10T13:27:42.841836Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 1.3719 - acc: 0.6176 - val_loss: 0.5019 - val_acc: 0.9450\n",
      "Epoch 2/4\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 1.3488 - acc: 0.6237 - val_loss: 0.4827 - val_acc: 0.9350\n",
      "Epoch 3/4\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 1.3219 - acc: 0.6430 - val_loss: 0.4658 - val_acc: 0.9440\n",
      "Epoch 4/4\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 1.3121 - acc: 0.6466 - val_loss: 0.4548 - val_acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2b121160>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, epochs=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a distinct improvement - even although the validation set isn't very big. This looks encouraging for when we try this on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:48.833943Z",
     "start_time": "2018-01-10T13:27:48.806243Z"
    }
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/bn-ps8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find a good clipping amount using the validation set, prior to submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:52.501307Z",
     "start_time": "2018-01-10T13:27:48.835457Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:52.706205Z",
     "start_time": "2018-01-10T13:27:52.503927Z"
    }
   },
   "outputs": [],
   "source": [
    "val_preds = bn_model.predict(conv_val_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:52.797210Z",
     "start_time": "2018-01-10T13:27:52.708009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46317427399009464"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:52.995260Z",
     "start_time": "2018-01-10T13:27:52.799357Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.113599Z",
     "start_time": "2018-01-10T13:27:52.996541Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_test_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.117054Z",
     "start_time": "2018-01-10T13:27:53.115102Z"
    }
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.152790Z",
     "start_time": "2018-01-10T13:27:53.118251Z"
    }
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.182014Z",
     "start_time": "2018-01-10T13:27:53.154128Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.233123Z",
     "start_time": "2018-01-10T13:27:53.183369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>own/img_10001.jpg</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.794290</td>\n",
       "      <td>0.036183</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.029527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>own/img_100228.jpg</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.148008</td>\n",
       "      <td>0.152710</td>\n",
       "      <td>0.016703</td>\n",
       "      <td>0.025873</td>\n",
       "      <td>0.035956</td>\n",
       "      <td>0.350845</td>\n",
       "      <td>0.057857</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>0.020536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>own/img_100259.jpg</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>0.216431</td>\n",
       "      <td>0.124052</td>\n",
       "      <td>0.057873</td>\n",
       "      <td>0.045128</td>\n",
       "      <td>0.042009</td>\n",
       "      <td>0.074831</td>\n",
       "      <td>0.173426</td>\n",
       "      <td>0.187613</td>\n",
       "      <td>0.041462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>own/img_100263.jpg</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.040189</td>\n",
       "      <td>0.014808</td>\n",
       "      <td>0.635093</td>\n",
       "      <td>0.185962</td>\n",
       "      <td>0.019687</td>\n",
       "      <td>0.011732</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.017345</td>\n",
       "      <td>0.022241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>own/img_100596.jpg</td>\n",
       "      <td>0.158790</td>\n",
       "      <td>0.217054</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>0.040138</td>\n",
       "      <td>0.026702</td>\n",
       "      <td>0.047620</td>\n",
       "      <td>0.068982</td>\n",
       "      <td>0.037552</td>\n",
       "      <td>0.074853</td>\n",
       "      <td>0.309085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  img        c0        c1        c2        c3        c4  \\\n",
       "0   own/img_10001.jpg  0.044445  0.027838  0.012445  0.007778  0.021270   \n",
       "1  own/img_100228.jpg  0.026350  0.148008  0.152710  0.016703  0.025873   \n",
       "2  own/img_100259.jpg  0.037174  0.216431  0.124052  0.057873  0.045128   \n",
       "3  own/img_100263.jpg  0.042274  0.040189  0.014808  0.635093  0.185962   \n",
       "4  own/img_100596.jpg  0.158790  0.217054  0.019222  0.040138  0.026702   \n",
       "\n",
       "         c5        c6        c7        c8        c9  \n",
       "0  0.794290  0.036183  0.007778  0.021229  0.029527  \n",
       "1  0.035956  0.350845  0.057857  0.165161  0.020536  \n",
       "2  0.042009  0.074831  0.173426  0.187613  0.041462  \n",
       "3  0.019687  0.011732  0.010670  0.017345  0.022241  \n",
       "4  0.047620  0.068982  0.037552  0.074853  0.309085  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.298178Z",
     "start_time": "2018-01-10T13:27:53.234497Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.302499Z",
     "start_time": "2018-01-10T13:27:53.299763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/state/sample/results/subm.gz' target='_blank'>data/state/sample/results/subm.gz</a><br>"
      ],
      "text/plain": [
       "/home/roebius/pj/f1/nbs/data/state/sample/results/subm.gz"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets 0.534 on the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"things that didn't really work\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can safely ignore everything from here on, because they didn't really help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune some conv layers too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.730592Z",
     "start_time": "2018-01-10T13:27:53.303746Z"
    }
   },
   "outputs": [],
   "source": [
    "#for l in get_bn_layers(p): conv_model.add(l)  #  this choice would give a weight shape error\n",
    "for l in get_bn_da_layers(p): conv_model.add(l)  # ... so probably this is the right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.742473Z",
     "start_time": "2018-01-10T13:27:53.732070Z"
    }
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(bn_model.layers, conv_model.layers[last_conv_idx+1:]):\n",
    "    l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.777382Z",
     "start_time": "2018-01-10T13:27:53.744000Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers: l.trainable =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:53.803074Z",
     "start_time": "2018-01-10T13:27:53.778702Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers[last_conv_idx+1:]: l.trainable =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.142967Z",
     "start_time": "2018-01-10T13:27:53.804452Z"
    }
   },
   "outputs": [],
   "source": [
    "comb = np.concatenate([trn, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.146638Z",
     "start_time": "2018-01-10T13:27:54.144489Z"
    }
   },
   "outputs": [],
   "source": [
    " # not knowing what the experiment was about, added this to avoid a shape match error with comb using gen_t.flow\n",
    "comb_pseudo = np.concatenate([trn_labels, val_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.178862Z",
     "start_time": "2018-01-10T13:27:54.147942Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=8, height_shift_range=0.04, \n",
    "                shear_range=0.03, channel_shift_range=10, width_shift_range=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.212027Z",
     "start_time": "2018-01-10T13:27:54.180537Z"
    }
   },
   "outputs": [],
   "source": [
    "batches = gen_t.flow(comb, comb_pseudo, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.345736Z",
     "start_time": "2018-01-10T13:27:54.213329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:27:54.364801Z",
     "start_time": "2018-01-10T13:27:54.347604Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:28:23.043281Z",
     "start_time": "2018-01-10T13:27:54.366766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "24/24 [==============================] - 23s 941ms/step - loss: 1.2991 - acc: 0.6966 - val_loss: 0.4551 - val_acc: 0.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2a8fc9b0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, steps_per_epoch, epochs=1, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:28:23.049567Z",
     "start_time": "2018-01-10T13:28:23.046229Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:29:30.589417Z",
     "start_time": "2018-01-10T13:28:23.052532Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "24/24 [==============================] - 23s 943ms/step - loss: 1.3306 - acc: 0.6947 - val_loss: 0.4558 - val_acc: 0.9370\n",
      "Epoch 2/3\n",
      "24/24 [==============================] - 23s 943ms/step - loss: 1.3471 - acc: 0.6934 - val_loss: 0.4539 - val_acc: 0.9350\n",
      "Epoch 3/3\n",
      "24/24 [==============================] - 22s 925ms/step - loss: 1.2838 - acc: 0.7064 - val_loss: 0.4546 - val_acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b280bea58>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, steps_per_epoch, epochs=3, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:29:30.596053Z",
     "start_time": "2018-01-10T13:29:30.592299Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers[16:]: l.trainable =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:29:30.638412Z",
     "start_time": "2018-01-10T13:29:30.599014Z"
    }
   },
   "outputs": [],
   "source": [
    "#- added compile instruction in order to avoid Keras 2.1 warning message\n",
    "conv_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:29:30.666532Z",
     "start_time": "2018-01-10T13:29:30.640149Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:31.322102Z",
     "start_time": "2018-01-10T13:29:30.667925Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "24/24 [==============================] - 36s 2s/step - loss: 1.2655 - acc: 0.7324 - val_loss: 0.2987 - val_acc: 0.9340\n",
      "Epoch 2/8\n",
      "24/24 [==============================] - 37s 2s/step - loss: 1.1985 - acc: 0.7615 - val_loss: 0.3260 - val_acc: 0.9320\n",
      "Epoch 3/8\n",
      "24/24 [==============================] - 36s 2s/step - loss: 1.2366 - acc: 0.7602 - val_loss: 0.3567 - val_acc: 0.9250\n",
      "Epoch 4/8\n",
      "24/24 [==============================] - 37s 2s/step - loss: 1.2386 - acc: 0.7669 - val_loss: 0.3153 - val_acc: 0.9390\n",
      "Epoch 5/8\n",
      "24/24 [==============================] - 36s 2s/step - loss: 1.1640 - acc: 0.7671 - val_loss: 0.3056 - val_acc: 0.9420\n",
      "Epoch 6/8\n",
      "24/24 [==============================] - 37s 2s/step - loss: 1.1472 - acc: 0.7910 - val_loss: 0.3079 - val_acc: 0.9420\n",
      "Epoch 7/8\n",
      "24/24 [==============================] - 35s 1s/step - loss: 1.1709 - acc: 0.8094 - val_loss: 0.3302 - val_acc: 0.9420\n",
      "Epoch 8/8\n",
      "24/24 [==============================] - 37s 2s/step - loss: 1.1433 - acc: 0.7949 - val_loss: 0.3212 - val_acc: 0.9430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2a0d0828>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, steps_per_epoch, epochs=8, validation_data=val_batches, \n",
    "                 validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:31.469902Z",
     "start_time": "2018-01-10T13:34:31.324913Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(path+'models/conv8_ps.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:31.473355Z",
     "start_time": "2018-01-10T13:34:31.471459Z"
    }
   },
   "outputs": [],
   "source": [
    "#conv_model.load_weights(path+'models/conv8_da.h5')  # conv8_da.h5 was not saved in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.615737Z",
     "start_time": "2018-01-10T13:34:31.474775Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_pseudo = conv_model.predict(val, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.620312Z",
     "start_time": "2018-01-10T13:34:40.617269Z"
    }
   },
   "outputs": [],
   "source": [
    "save_array(path+'models/pseudo8_da.dat', val_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.689903Z",
     "start_time": "2018-01-10T13:34:40.622952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_ds = pd.read_csv(path+'driver_imgs_list.csv')\n",
    "drivers_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.702450Z",
     "start_time": "2018-01-10T13:34:40.692075Z"
    }
   },
   "outputs": [],
   "source": [
    "img2driver = drivers_ds.set_index('img')['subject'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.735686Z",
     "start_time": "2018-01-10T13:34:40.704227Z"
    }
   },
   "outputs": [],
   "source": [
    "driver2imgs = {k: g[\"img\"].tolist() \n",
    "               for k,g in drivers_ds[['subject', 'img']].groupby(\"subject\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.751624Z",
     "start_time": "2018-01-10T13:34:40.737364Z"
    }
   },
   "outputs": [],
   "source": [
    "# It seems this function is not used in this notebook\n",
    "def get_idx(driver_list):\n",
    "    return [i for i,f in enumerate(filenames) if img2driver[f[3:]] in driver_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.782489Z",
     "start_time": "2018-01-10T13:34:40.753229Z"
    }
   },
   "outputs": [],
   "source": [
    "# drivers = driver2imgs.keys()  # Python 2\n",
    "drivers = list(driver2imgs)  # Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.812102Z",
     "start_time": "2018-01-10T13:34:40.784975Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_drivers = np.random.permutation(drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T13:34:40.842883Z",
     "start_time": "2018-01-10T13:34:40.813919Z"
    }
   },
   "outputs": [],
   "source": [
    "ds1 = rnd_drivers[:len(rnd_drivers)//2]\n",
    "ds2 = rnd_drivers[len(rnd_drivers)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:58:33.336528Z",
     "start_time": "2018-01-10T12:58:33.280165Z"
    }
   },
   "outputs": [],
   "source": [
    "# The following cells seem to require some preparation code not included in this notebook\n",
    "models=[fit_conv([d]) for d in drivers]\n",
    "models=[m for m in models if m is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:58:33.337411Z",
     "start_time": "2018-01-10T12:37:52.242Z"
    }
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(conv_test_feat, batch_size=128) for m in models])\n",
    "avg_preds = all_preds.mean(axis=0)\n",
    "avg_preds = avg_preds/np.expand_dims(avg_preds.sum(axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:58:33.338389Z",
     "start_time": "2018-01-10T12:37:52.243Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, np.clip(avg_val_preds,0.01,0.99)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:58:33.339357Z",
     "start_time": "2018-01-10T12:37:52.245Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.metrics.categorical_accuracy(val_labels, np.clip(avg_val_preds,0.01,0.99)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
